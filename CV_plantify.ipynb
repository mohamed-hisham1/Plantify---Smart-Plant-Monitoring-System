{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"soumiknafiul/plantvillage-dataset-labeled\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "EVFo20muslM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab3c1cc-161e-4490-9cd1-84f7db4e21ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'plantvillage-dataset-labeled' dataset.\n",
            "Path to dataset files: /kaggle/input/plantvillage-dataset-labeled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow tensorflow-addons scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWF4MPuLya_s",
        "outputId": "4e136309-f58c-4df8-cfcc-8b731a04fc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-addons\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1L920A9qzujF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"/kaggle/input/plantvillage-dataset-labeled/PlantVillage Dataset (Labeled)\")  # ← عدّله لو مكان مختلف\n",
        "\n",
        "variants = [\n",
        "    \"Color Images\",\n",
        "    \"Grayscale Images\",\n",
        "    \"Segmented Images\"\n",
        "]\n",
        "\n",
        "available = [v for v in variants if (ROOT / v).exists()]\n",
        "print(\"Found variants:\", available)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJcIz9cv3hVb",
        "outputId": "d563c816-2227-4398-af91-e97dd5282649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found variants: ['Color Images', 'Grayscale Images', 'Segmented Images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gather_variant_paths(variant):\n",
        "    root = ROOT / variant\n",
        "    class_dirs = [p for p in sorted(root.iterdir()) if p.is_dir()]\n",
        "    class_names = [p.name for p in class_dirs]\n",
        "    paths, labels = [], []\n",
        "\n",
        "    for idx, c in enumerate(class_dirs):\n",
        "        for img in c.glob(\"*\"):\n",
        "            if img.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "                paths.append(str(img))\n",
        "                labels.append(idx)\n",
        "\n",
        "    return {\n",
        "        \"variant\": variant,\n",
        "        \"root\": str(root),\n",
        "        \"class_dirs\": class_dirs,\n",
        "        \"class_names\": class_names,\n",
        "        \"paths\": paths,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "data_info = {}\n",
        "for v in available:\n",
        "    info = gather_variant_paths(v)\n",
        "    print(f\"{v}: classes={len(info['class_names'])}, images={len(info['paths'])}\")\n",
        "    data_info[v] = info\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meyZR2kwzzAz",
        "outputId": "cdf60d0f-0ff6-421f-9076-22f48a303836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Color Images: classes=19, images=15915\n",
            "Grayscale Images: classes=25, images=27023\n",
            "Segmented Images: classes=18, images=17842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "split_info = {}\n",
        "seed = 123\n",
        "\n",
        "for v, info in data_info.items():\n",
        "    paths = np.array(info[\"paths\"])\n",
        "    labels = np.array(info[\"labels\"])\n",
        "\n",
        "    # stratified split 80/10/10\n",
        "    train_paths, rest_paths, train_labels, rest_labels = train_test_split(\n",
        "        paths, labels, test_size=0.2, random_state=seed, stratify=labels\n",
        "    )\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        rest_paths, rest_labels, test_size=0.5, random_state=seed, stratify=rest_labels\n",
        "    )\n",
        "\n",
        "    split_info[v] = {\n",
        "        \"train_paths\": train_paths,\n",
        "        \"train_labels\": train_labels,\n",
        "        \"val_paths\": val_paths,\n",
        "        \"val_labels\": val_labels,\n",
        "        \"test_paths\": test_paths,\n",
        "        \"test_labels\": test_labels,\n",
        "        \"class_names\": info[\"class_names\"]\n",
        "    }\n",
        "\n",
        "    print(f\"{v} → Train={len(train_paths)}, Val={len(val_paths)}, Test={len(test_paths)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bm5Aa_Wz0ql",
        "outputId": "58b4b2aa-81c4-4a3a-b09a-e04485976b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Color Images → Train=12732, Val=1591, Test=1592\n",
            "Grayscale Images → Train=21618, Val=2702, Test=2703\n",
            "Segmented Images → Train=14273, Val=1784, Test=1785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def preprocess_path(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "def make_dataset(paths, labels, shuffle=False, augment=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(paths), seed=seed)\n",
        "    ds = ds.map(preprocess_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        aug = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "            tf.keras.layers.RandomRotation(0.2),\n",
        "            tf.keras.layers.RandomZoom(0.15),\n",
        "        ])\n",
        "        ds = ds.map(lambda x, y: (aug(x), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "HOVj5Mm139ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "def build_model(num_classes):\n",
        "    base = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=IMG_SIZE+(3,))\n",
        "    base.trainable = False\n",
        "\n",
        "    inputs = Input(shape=IMG_SIZE+(3,))\n",
        "    x = base(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Vnz72Xqx4mld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {}\n",
        "\n",
        "for v, sp in split_info.items():\n",
        "    print(\"\\n==== Training:\", v, \"====\")\n",
        "\n",
        "    train_ds = make_dataset(sp[\"train_paths\"], sp[\"train_labels\"], augment=False)\n",
        "    val_ds = make_dataset(sp[\"val_paths\"], sp[\"val_labels\"], shuffle=False)\n",
        "\n",
        "    num_classes = len(sp[\"class_names\"])\n",
        "    model = build_model(num_classes)\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=12, callbacks=callbacks)\n",
        "\n",
        "    models[v] = {\n",
        "        \"model\": model,\n",
        "        \"class_names\": sp[\"class_names\"]\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOcVszBl4odj",
        "outputId": "a9dae098-244f-4cb1-9e6b-a23044472c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Training: Color Images ====\n",
            "Epoch 1/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 180ms/step - accuracy: 0.6779 - loss: 1.2256 - val_accuracy: 0.9415 - val_loss: 0.2507\n",
            "Epoch 2/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 52ms/step - accuracy: 0.9362 - loss: 0.2607 - val_accuracy: 0.9623 - val_loss: 0.1578\n",
            "Epoch 3/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 52ms/step - accuracy: 0.9537 - loss: 0.1781 - val_accuracy: 0.9667 - val_loss: 0.1274\n",
            "Epoch 4/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 52ms/step - accuracy: 0.9634 - loss: 0.1396 - val_accuracy: 0.9705 - val_loss: 0.1076\n",
            "Epoch 5/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.9649 - loss: 0.1241 - val_accuracy: 0.9730 - val_loss: 0.0960\n",
            "Epoch 6/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 56ms/step - accuracy: 0.9722 - loss: 0.1051 - val_accuracy: 0.9749 - val_loss: 0.0882\n",
            "Epoch 7/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - accuracy: 0.9761 - loss: 0.0946 - val_accuracy: 0.9749 - val_loss: 0.0796\n",
            "Epoch 8/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.9762 - loss: 0.0877 - val_accuracy: 0.9767 - val_loss: 0.0766\n",
            "Epoch 9/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 56ms/step - accuracy: 0.9782 - loss: 0.0786 - val_accuracy: 0.9793 - val_loss: 0.0729\n",
            "Epoch 10/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 59ms/step - accuracy: 0.9771 - loss: 0.0793 - val_accuracy: 0.9805 - val_loss: 0.0679\n",
            "Epoch 11/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 49ms/step - accuracy: 0.9782 - loss: 0.0698 - val_accuracy: 0.9786 - val_loss: 0.0628\n",
            "Epoch 12/12\n",
            "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - accuracy: 0.9795 - loss: 0.0686 - val_accuracy: 0.9818 - val_loss: 0.0648\n",
            "\n",
            "==== Training: Grayscale Images ====\n",
            "Epoch 1/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 133ms/step - accuracy: 0.6628 - loss: 1.2285 - val_accuracy: 0.8934 - val_loss: 0.3796\n",
            "Epoch 2/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 52ms/step - accuracy: 0.8821 - loss: 0.3889 - val_accuracy: 0.9101 - val_loss: 0.2935\n",
            "Epoch 3/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9046 - loss: 0.3035 - val_accuracy: 0.9249 - val_loss: 0.2533\n",
            "Epoch 4/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - accuracy: 0.9188 - loss: 0.2596 - val_accuracy: 0.9293 - val_loss: 0.2284\n",
            "Epoch 5/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - accuracy: 0.9217 - loss: 0.2404 - val_accuracy: 0.9345 - val_loss: 0.2073\n",
            "Epoch 6/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.9277 - loss: 0.2223 - val_accuracy: 0.9397 - val_loss: 0.2052\n",
            "Epoch 7/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.9330 - loss: 0.2113 - val_accuracy: 0.9400 - val_loss: 0.2018\n",
            "Epoch 8/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 51ms/step - accuracy: 0.9341 - loss: 0.2011 - val_accuracy: 0.9415 - val_loss: 0.1881\n",
            "Epoch 9/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - accuracy: 0.9376 - loss: 0.1908 - val_accuracy: 0.9419 - val_loss: 0.1850\n",
            "Epoch 10/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - accuracy: 0.9370 - loss: 0.1851 - val_accuracy: 0.9412 - val_loss: 0.1862\n",
            "Epoch 11/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.9396 - loss: 0.1774 - val_accuracy: 0.9449 - val_loss: 0.1767\n",
            "Epoch 12/12\n",
            "\u001b[1m676/676\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 53ms/step - accuracy: 0.9420 - loss: 0.1752 - val_accuracy: 0.9441 - val_loss: 0.1837\n",
            "\n",
            "==== Training: Segmented Images ====\n",
            "Epoch 1/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 156ms/step - accuracy: 0.6488 - loss: 1.2972 - val_accuracy: 0.9193 - val_loss: 0.3396\n",
            "Epoch 2/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 48ms/step - accuracy: 0.9168 - loss: 0.3483 - val_accuracy: 0.9445 - val_loss: 0.2195\n",
            "Epoch 3/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 49ms/step - accuracy: 0.9374 - loss: 0.2470 - val_accuracy: 0.9501 - val_loss: 0.1743\n",
            "Epoch 4/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 48ms/step - accuracy: 0.9480 - loss: 0.2008 - val_accuracy: 0.9596 - val_loss: 0.1459\n",
            "Epoch 5/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 50ms/step - accuracy: 0.9533 - loss: 0.1754 - val_accuracy: 0.9647 - val_loss: 0.1283\n",
            "Epoch 6/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 48ms/step - accuracy: 0.9560 - loss: 0.1536 - val_accuracy: 0.9686 - val_loss: 0.1163\n",
            "Epoch 7/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.9578 - loss: 0.1457 - val_accuracy: 0.9742 - val_loss: 0.1035\n",
            "Epoch 8/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 49ms/step - accuracy: 0.9638 - loss: 0.1318 - val_accuracy: 0.9697 - val_loss: 0.1022\n",
            "Epoch 9/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 49ms/step - accuracy: 0.9649 - loss: 0.1245 - val_accuracy: 0.9720 - val_loss: 0.0975\n",
            "Epoch 10/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 50ms/step - accuracy: 0.9666 - loss: 0.1178 - val_accuracy: 0.9731 - val_loss: 0.0902\n",
            "Epoch 11/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 49ms/step - accuracy: 0.9694 - loss: 0.1071 - val_accuracy: 0.9748 - val_loss: 0.0860\n",
            "Epoch 12/12\n",
            "\u001b[1m447/447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.9673 - loss: 0.1077 - val_accuracy: 0.9742 - val_loss: 0.0850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = predict(\"/Himachal-Pradesh-apple-scab-photos.jpg\", variant=\"Color Images\")\n",
        "print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hknaL3tc9xoz",
        "outputId": "1f14240d-62dc-4c40-ce82-8dccaf3385f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "{'variant': 'Color Images', 'label': 'diseased', 'disease_name': 'Apple scab', 'probability': 0.813114583492279}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# فولدر التخزين\n",
        "SAVE_DIR = Path(\"/content/saved_models\")\n",
        "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# حفظ كل موديل\n",
        "for variant, entry in models.items():\n",
        "    model = entry[\"model\"]\n",
        "\n",
        "    variant_dir = SAVE_DIR / variant.replace(\" \", \"_\")\n",
        "    variant_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    model_path = variant_dir / \"model.h5\"\n",
        "    model.save(model_path)\n",
        "\n",
        "    print(f\"[✓] Saved model for {variant} → {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yVbWPxtDfNz",
        "outputId": "20fc6eb8-52d0-434f-9665-af9b7343af13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✓] Saved model for Color Images → /content/saved_models/Color_Images/model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✓] Saved model for Grayscale Images → /content/saved_models/Grayscale_Images/model.h5\n",
            "[✓] Saved model for Segmented Images → /content/saved_models/Segmented_Images/model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Nxc4MAsD9XE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}